{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:21.435928Z",
     "start_time": "2018-11-15T21:09:18.002851Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음 시도한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url로 접근\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless') \n",
    "options.add_argument('window-size = 1920x1980')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://news.nate.com')\n",
    "\n",
    "driver.find_element_by_id('input_search').send_keys('아모레퍼시픽')\n",
    "\n",
    "driver.find_element_by_id('acBtn').click()\n",
    "\n",
    "iframe = driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe')\n",
    "driver.switch_to_frame(iframe)\n",
    "\n",
    "#driver.find_element_by_css_selector('dl.option-list > dd.opt02 > span.sel02').click() # 검색영역 '제목' 선택\n",
    "\n",
    "# 기간 설정\n",
    "driver.find_element_by_name('ps1').clear()\n",
    "driver.find_element_by_name('ps1').send_keys('20100806')\n",
    "driver.find_element_by_name('ps2').clear()\n",
    "driver.find_element_by_name('ps2').send_keys('20180912')\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "driver.find_element_by_class_name('type-search-btn').click()\n",
    "justin = 0\n",
    "while True:\n",
    "    try:\n",
    "        if driver.find_element_by_css_selector('div.none-result'):\n",
    "            driver.refresh()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "time.sleep(1)\n",
    "\n",
    "lastdate = datetime.datetime.strptime('20180912', '%Y%m%d')\n",
    "url_list = []\n",
    "p = 20\n",
    "while True:\n",
    "    print('jjjjjjjjjjjjj : ', justin)\n",
    "    print('While문 진입')\n",
    "    try:\n",
    "        #if driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe'):\n",
    "        iframe = driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe')\n",
    "        driver.switch_to_frame(iframe)\n",
    "        page_url = driver.find_element_by_css_selector('div.paging > span > a').get_attribute('href')\n",
    "        page_url = page_url[:-2]\n",
    "        for i in range(p):\n",
    "            try:\n",
    "                URL = page_url + str(1 + (i * 10))\n",
    "                driver.get(URL)\n",
    "                time.sleep(1)\n",
    "                titles = driver.find_elements_by_css_selector('dt.text-inline')\n",
    "                \n",
    "                if p > 10:\n",
    "                    print(i, '   이건데 왜 이거 실행함?')\n",
    "                    dates = [day.text[-16:-6].replace('.', '') for day in driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')]\n",
    "                    dtime = [datetime.datetime.strptime(d, '%Y%m%d') for d in dates]\n",
    "                    if lastdate > min(dtime):\n",
    "                        lastdate = min(dtime)\n",
    "                    print(lastdate + '   *' * 100)                \n",
    "                \n",
    "                for j in range(len(titles)):\n",
    "                    url_list.append(titles[j].find_element_by_tag_name('a').get_attribute('href').replace('https://news.nate.com/view/', ''))\n",
    "                if i == 99:\n",
    "                    print('진행상황 ({}/99)'.format(i + justin*100))\n",
    "                    pd.Series(url_list).to_csv('news_data/news{}.csv'.format(i + justin * 100), index = False)\n",
    "                    url_list = []\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('이번 페이지는 크롤링 못 했다. (%d페이지)' %(i+1))\n",
    "                URL = page_url + str(1 + (i * 10))\n",
    "                driver.get(URL)\n",
    "                time.sleep(1)\n",
    "                titles = driver.find_elements_by_css_selector('dt.text-inline')\n",
    "                \n",
    "                if p > 10:\n",
    "                    print(i, '   이건데 왜 이거 실행함?')                    \n",
    "                    dates = [day.text[-16:-6].replace('.', '') for day in driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')]\n",
    "                    dtime = [datetime.datetime.strptime(d, '%Y%m%d') for d in dates]\n",
    "                    if lastdate > min(dtime):\n",
    "                        lastdate = min(dtime)\n",
    "                    print(lastdate + '   *' * 100)\n",
    "                    \n",
    "                for j in range(len(titles)):\n",
    "                    url_list.append(titles[j].find_element_by_tag_name('a').get_attribute('href').replace('https://news.nate.com/view/', ''))\n",
    "                if i == 99:\n",
    "                    print('진행상황 ({}/99)'.format(i + justin * 100))\n",
    "                    pd.Series(url_list).to_csv('news_data/news{}.csv'.format(i + justin * 100), index = False)\n",
    "                    url_list = []\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('No iframe')\n",
    "        #iframe = driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe')\n",
    "        #driver.switch_to_frame(iframe)\n",
    "        page_url = driver.find_element_by_css_selector('div.paging > span > a').get_attribute('href')\n",
    "        page_url = page_url[:-2]\n",
    "        for i in range(p):\n",
    "            try:\n",
    "                print(i,' *' * 100)\n",
    "                URL = page_url + str(1 + (i * 10))\n",
    "                driver.get(URL)\n",
    "                time.sleep(1)\n",
    "                titles = driver.find_elements_by_css_selector('dt.text-inline')\n",
    "                \n",
    "                if p > 10:                \n",
    "                    dates = [day.text[-16:-6].replace('.', '') for day in driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')]\n",
    "                    dtime = [datetime.datetime.strptime(d, '%Y%m%d') for d in dates]\n",
    "                    if lastdate > min(dtime):\n",
    "                        lastdate = min(dtime)\n",
    "                    print(lastdate, '   *' * 100)                \n",
    "                \n",
    "                for j in range(len(titles)):\n",
    "                    url_list.append(titles[j].find_element_by_tag_name('a').get_attribute('href').replace('https://news.nate.com/view/', ''))\n",
    "                if  i == 99:\n",
    "                    print('진행상황 ({}/99)'.format(i + justin * 100))\n",
    "                    pd.Series(url_list).to_csv('news_data/news{}.csv'.format(i + justin*100), index = False)\n",
    "                    url_list = []\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(i,' * ' * 100)                \n",
    "                print(e)\n",
    "                print('이번 페이지는 크롤링 못 했다. (%d페이지)' %(i+1))\n",
    "                URL = page_url + str(1 + (i * 10))\n",
    "                driver.get(URL)\n",
    "                time.sleep(1)\n",
    "                titles = driver.find_elements_by_css_selector('dt.text-inline')\n",
    "                \n",
    "                if p > 10:                \n",
    "                    dates = [day.text[-16:-6].replace('.', '') for day in driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')]\n",
    "                    dtime = [datetime.datetime.strptime(d, '%Y%m%d') for d in dates]\n",
    "                    if lastdate > min(dtime):\n",
    "                        lastdate = min(dtime)\n",
    "                    print(lastdate, '   *' * 100)                \n",
    "                \n",
    "                for j in range(len(titles)):\n",
    "                    url_list.append(titles[j].find_element_by_tag_name('a').get_attribute('href').replace('https://news.nate.com/view/', ''))\n",
    "                if  i == 99:\n",
    "                    print('진행상황 ({}/99)'.format(i + justin * 100))\n",
    "                    pd.Series(url_list).to_csv('news_data/news{}.csv'.format(i + justin * 100), index = False)\n",
    "                    url_list = []\n",
    "        \n",
    "    # 마지막 페이지의 마지막 글의 날짜\n",
    "    #date_list = driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')\n",
    "    #date = date_list[9].text[-16:-6].replace('.', '')\n",
    "    \n",
    "    dateinput = lastdate.strftime('%Y%m%d')\n",
    "    driver.find_element_by_name('ps1').clear()\n",
    "    driver.find_element_by_name('ps1').send_keys('20100806')\n",
    "    driver.find_element_by_name('ps2').clear()\n",
    "    driver.find_element_by_name('ps2').send_keys(dateinput)\n",
    "    time.sleep(2)\n",
    "    print(dateinput)\n",
    "    #driver.find_element_by_css_selector('dl.option-list > dd.opt02 > span.sel02').click() # 검색영역 '제목' 선택\n",
    "    driver.find_element_by_class_name('type-search-btn').click()\n",
    "    print('endendendendenden : ', justin)\n",
    "    justin = justin + 1\n",
    "    if justin == 123:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로 도전(성공!)\n",
    "- 날짜를 7일마다 변경하여 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# url로 접근\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless') \n",
    "options.add_argument('window-size = 1920x1980')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://news.nate.com')\n",
    "\n",
    "driver.find_element_by_id('input_search').send_keys('아모레퍼시픽')\n",
    "\n",
    "driver.find_element_by_id('acBtn').click()\n",
    "\n",
    "iframe = driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe')\n",
    "driver.switch_to_frame(iframe)\n",
    "\n",
    "#driver.find_element_by_css_selector('dl.option-list > dd.opt02 > span.sel02').click() # 검색영역 '제목' 선택\n",
    "\n",
    "# 기간 설정\n",
    "driver.find_element_by_name('ps1').clear()\n",
    "driver.find_element_by_name('ps1').send_keys('20180905')\n",
    "driver.find_element_by_name('ps2').clear()\n",
    "driver.find_element_by_name('ps2').send_keys('20180912')\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "driver.find_element_by_class_name('type-search-btn').click()\n",
    "time.sleep(5)\n",
    "\n",
    "justin = 0\n",
    "start_date = '20180912'\n",
    "middle_date = '20180905'\n",
    "finish_date = '20100806'\n",
    "delta = -7\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if driver.find_element_by_css_selector('div.none-result'):\n",
    "            driver.refresh()\n",
    "            time.sleep(3)\n",
    "            print('검색 클릭 후 창 출력 실패 -> refresh')\n",
    "            ### 여기가 문제구먼 날짜가 바뀌는 부분\n",
    "            driver.find_element_by_name('ps1').clear()\n",
    "            driver.find_element_by_name('ps1').send_keys(middle_date)\n",
    "            driver.find_element_by_name('ps2').clear()\n",
    "            driver.find_element_by_name('ps2').send_keys(start_date)\n",
    "            driver.find_element_by_class_name('type-search-btn').click()\n",
    "    except Exception as e:\n",
    "        print('페이지가 정상적으로 출력')\n",
    "        break\n",
    "time.sleep(1)\n",
    "\n",
    "stop = 0\n",
    "while True:\n",
    "    try:\n",
    "        iframe = driver.find_element_by_id('contentsWraper').find_element_by_tag_name('iframe')\n",
    "        driver.switch_to_frame(iframe)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print('iframe이 이미 있습니다.')\n",
    "        break\n",
    "    stop = stop + 1\n",
    "    if stop == 10:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "try:\n",
    "    element = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.paging > span > a'))\n",
    "    )\n",
    "except:\n",
    "    print('시작이 안 좋네')    \n",
    "page_url = driver.find_element_by_css_selector('div.paging > span > a').get_attribute('href')\n",
    "page_url = page_url[:-2]\n",
    "\n",
    "i = 1\n",
    "url_list = []\n",
    "stop_hard = 1\n",
    "justin = 1\n",
    "while True:\n",
    "    try:\n",
    "        \n",
    "        URL = page_url + str(10 * i - 9)\n",
    "        driver.get(URL)\n",
    "        \n",
    "        # 마지막 페이지 확인 후 다시 처음으로\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div.paging'))\n",
    "            )\n",
    "        except:\n",
    "            print('도저히 페이지 누루는 부분을 못 찾네')\n",
    "            drver.refresh()\n",
    "            \n",
    "        last_URL = driver.find_element_by_link_text('마지막').get_attribute('href')\n",
    "            \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'dt.text-inline'))\n",
    "            )\n",
    "        except:\n",
    "            print('도저히 이 element를 못 찾네 ', str(10 * i - 9))\n",
    "            driver.refresh()\n",
    "         \n",
    "        \n",
    "        titles = driver.find_elements_by_css_selector('dt.text-inline')\n",
    "        dates = [day.text[-16:-6].replace('.', '') for day in driver.find_elements_by_css_selector('ul.search-list dl dd.text-inline span')]\n",
    "        print(len(titles), ' ', URL)\n",
    "        \n",
    "        if len(titles) == 0:\n",
    "            raise print('이 부분은 title이 0개', URL)\n",
    "            \n",
    "        for j in range(len(titles)):\n",
    "            url_list.append(titles[j].find_element_by_tag_name('a').get_attribute('href').replace('https://news.nate.com/view/', ''))\n",
    "            \n",
    "        if True:\n",
    "            print('\\r{}/13000'.format(i + justin * 100 - 100), end = '')\n",
    "            print('진행상황 ({}/125000)'.format(i * justin * 10), 'url 개수 = %d' %(len(url_list)))\n",
    "            # pd.Series(url_list).to_csv('news_data/news{}.csv'.format(justin), index = False)\n",
    "            \n",
    " \n",
    "            \n",
    "        i = i + 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        print('%d 번 페이지는 크롤링 못 함, refresh 필요' %i)\n",
    "        None\n",
    "\n",
    "    if URL == page_url + str(int(last_URL[-3:]) + 10):\n",
    "        print('끝났습니다. 정상적으로 크롤링되었습니다. %s' %middle_date)\n",
    "        print('')\n",
    "        \n",
    "        # dates = {day for d in url_list for day in re.findall('([0-9]+)n', d)}\n",
    "        # date_format = [datetime.strptime(dd, '%Y%m%d') for dd in dates]\n",
    "        # date_input = min(date_format)\n",
    "        \n",
    "        start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "        middle_date = datetime.strptime(middle_date, '%Y%m%d')\n",
    "        start_date = middle_date\n",
    "        middle_date = middle_date - timedelta(days = 7)                \n",
    "        \n",
    "        start_date = start_date.strftime('%Y%m%d')\n",
    "        middle_date = middle_date.strftime('%Y%m%d')\n",
    "        print(start_date, '->', middle_date)\n",
    "        \n",
    "        driver.find_element_by_name('ps1').clear()\n",
    "        driver.find_element_by_name('ps1').send_keys(middle_date)\n",
    "        driver.find_element_by_name('ps2').clear()\n",
    "        driver.find_element_by_name('ps2').send_keys(start_date)\n",
    "        time.sleep(0.5)\n",
    "        driver.find_element_by_class_name('type-search-btn').click()\n",
    "        time.sleep(5)        \n",
    "        while True:\n",
    "            try:\n",
    "                if driver.find_element_by_css_selector('div.none-result'):\n",
    "                    driver.find_element_by_name('ps1').clear()\n",
    "                    driver.find_element_by_name('ps1').send_keys(finish_date)\n",
    "                    driver.find_element_by_name('ps2').clear()\n",
    "                    driver.find_element_by_name('ps2').send_keys(start_date)\n",
    "                    time.sleep(0.5)\n",
    "                    driver.find_element_by_class_name('type-search-btn').click()\n",
    "                    time.sleep(3)\n",
    "            except:\n",
    "                print('날짜 변경 시 오류 없음')\n",
    "                try:\n",
    "                    element = WebDriverWait(driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.paging > span > a'))\n",
    "                    )\n",
    "                    page_url = driver.find_element_by_css_selector('div.paging > span > a').get_attribute('href')\n",
    "                    page_url = page_url[:-2]\n",
    "                except:\n",
    "                    print('실제로 날짜는 안 바뀜')\n",
    "                    None\n",
    "                break\n",
    "        pd.Series(url_list).to_csv('news_data/news{}.csv'.format(justin), index = False)\n",
    "        url_list = []\n",
    "        i = 1\n",
    "        justin += 1\n",
    "        \n",
    "    stop_hard += 1\n",
    "    if stop_hard == 1000000 | justin == 130:\n",
    "        print('오류가 발생했습니다. 강제로 종류합니다.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL 추출 성공 후 내용 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://news.nate.com/view/'\n",
    "\n",
    "\n",
    "# all_files = glob.glob(path_input + '/*.csv')\n",
    "# df_list = [pd.read_csv(file, encoding = 'utf-8', header = None) for file in all_files]\n",
    "# df = pd.concat(df_list, sort = False)\n",
    "# df.columns = ['URL']\n",
    "df = pd.read_csv('news_url(20160928-20180912).csv', encoding = 'utf-8')\n",
    "\n",
    "df.drop_duplicates('URL', keep = 'first', inplace = True)\n",
    "url_list = list(set(df['URL'].values))\n",
    "list_len = len(url_list)\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# 1000개씩 자르자\n",
    "i = 1\n",
    "\n",
    "title_list = []\n",
    "content_list = []\n",
    "date_list = []\n",
    "error_URL = []\n",
    "\n",
    "\n",
    "for j, u in enumerate(url_list):\n",
    "    URL = url + u\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_all_elements_located((By.ID, \"articleView\"))\n",
    "        )\n",
    "    except:\n",
    "        print('해당 URL오류 발생 -> 따로 추가했음(error_URL)')\n",
    "        error_URL.append(URL)\n",
    "    \n",
    "    try:\n",
    "        title = driver.find_element_by_class_name('articleSubecjt').text # 제목\n",
    "        date = driver.find_element_by_css_selector('.firstDate em').text # 날짜\n",
    "        content = driver.find_element_by_id('realArtcContents').text # 내용\n",
    "        \n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        date_list.append(date)\n",
    "        \n",
    "    except:\n",
    "        title = None\n",
    "        date = None\n",
    "        content = None\n",
    "        error_URL.append(URL)\n",
    "        \n",
    "    if len(title_list) == 1000: # 원래는 1000\n",
    "        \n",
    "        data = {'title' : title_list, 'content' : content_list, 'date' : date_list}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('news_content{}.csv'.format(i * 1000), index = False, encoding = 'utf-8')\n",
    "        print('\\r 파일저장 현황 : {}/29'.format(i), end = '')\n",
    "        \n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        date_list = []\n",
    "    \n",
    "        i += 1\n",
    "        \n",
    "    print('\\r 진행상황 : {}/29260'.format(j + 1), end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중간에 끊겼을 때!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T02:27:24.261512Z",
     "start_time": "2018-11-16T22:43:33.568009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 진행상황 : 24127/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 24297/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 25013/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 25441/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 25493/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 26420/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 26478/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 27001/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 27810/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 28875/29260해당 URL오류 발생 -> 따로 추가했음(error_URL)\n",
      " 진행상황 : 29260/29260"
     ]
    }
   ],
   "source": [
    "start = 23551 # 진행상황과 같은 숫자 입력 ex) 15/29260 -> 15입력\n",
    "\n",
    "for j, u in enumerate(url_list[start:]):\n",
    "    URL = url + u\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_all_elements_located((By.ID, \"articleView\"))\n",
    "        )\n",
    "    except:\n",
    "        print('해당 URL오류 발생 -> 따로 추가했음(error_URL)')\n",
    "        error_URL.append(URL)\n",
    "    \n",
    "    try:\n",
    "        title = driver.find_element_by_class_name('articleSubecjt').text # 제목\n",
    "        date = driver.find_element_by_css_selector('.firstDate em').text # 날짜\n",
    "        content = driver.find_element_by_id('realArtcContents').text # 내용\n",
    "        \n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        date_list.append(date)\n",
    "        \n",
    "    except:\n",
    "        title = None\n",
    "        date = None\n",
    "        content = None\n",
    "        error_URL.append(URL)\n",
    "        \n",
    "    if len(title_list) == 1000: # 원래는 1000\n",
    "        \n",
    "        data = {'title' : title_list, 'content' : content_list, 'date' : date_list}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('news_content{}.csv'.format(i * 1000), index = False, encoding = 'utf-8')\n",
    "        print('\\r 파일저장 현황 : {}/29'.format(i), end = '')\n",
    "        \n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        date_list = []\n",
    "    \n",
    "        i += 1\n",
    "        \n",
    "    print('\\r 진행상황 : {}/29260'.format(start + j + 1), end = '')\n",
    "    if URL == url + url_list[-1]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롬창이 닫혔을 때!\n",
    "- 밑에 코드 먼저 실행하고 위에 다시 실행~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특수하게 쥬피터까지 꺼졌을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T05:49:45.701294Z",
     "start_time": "2018-11-15T21:11:00.792113Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://news.nate.com/view/'\n",
    "\n",
    "\n",
    "# all_files = glob.glob(path_input + '/*.csv')\n",
    "# df_list = [pd.read_csv(file, encoding = 'utf-8', header = None) for file in all_files]\n",
    "# df = pd.concat(df_list, sort = False)\n",
    "# df.columns = ['URL']\n",
    "df = pd.read_csv('news_url(20160928-20180912).csv', encoding = 'utf-8')\n",
    "\n",
    "df.drop_duplicates('URL', keep = 'first', inplace = True)\n",
    "url_list = list(set(df['URL'].values))\n",
    "list_len = len(url_list)\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# 1000개씩 자르자\n",
    "i = 1\n",
    "\n",
    "title_list = []\n",
    "content_list = []\n",
    "date_list = []\n",
    "error_URL = []\n",
    "\n",
    "start = 1000\n",
    "for j, u in enumerate(url_list[start:]):\n",
    "    URL = url + u\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_all_elements_located((By.ID, \"articleView\"))\n",
    "        )\n",
    "    except:\n",
    "        print('해당 URL오류 발생 -> 따로 추가했음(error_URL)')\n",
    "        error_URL.append(URL)\n",
    "    \n",
    "    try:\n",
    "        title = driver.find_element_by_class_name('articleSubecjt').text # 제목\n",
    "        date = driver.find_element_by_css_selector('.firstDate em').text # 날짜\n",
    "        content = driver.find_element_by_id('realArtcContents').text # 내용\n",
    "        \n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        date_list.append(date)\n",
    "        \n",
    "    except:\n",
    "        title = None\n",
    "        date = None\n",
    "        content = None\n",
    "        error_URL.append(URL)\n",
    "        \n",
    "    if len(title_list) == 1000: # 원래는 1000\n",
    "        \n",
    "        data = {'title' : title_list, 'content' : content_list, 'date' : date_list}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('news_content{}.csv'.format(i * 1000), index = False, encoding = 'utf-8')\n",
    "        print('\\r 파일저장 현황 : {}/29'.format(i), end = '')\n",
    "        \n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        date_list = []\n",
    "    \n",
    "        i += 1\n",
    "        \n",
    "    print('\\r 진행상황 : {}/29260'.format(start + j + 1), end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로 배운 사실\n",
    "1. try except은 혁명이다.\n",
    "2. 크롤링을 못 하도록 수법을 걸어놨는데, iframe페이지로 들어가니 상관 없어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오류\n",
    "- 초반 1000개 없어짐, 다시 해야 함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "993px",
    "right": "20px",
    "top": "58px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
