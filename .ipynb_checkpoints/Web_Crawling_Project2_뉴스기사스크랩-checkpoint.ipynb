{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling Project 2\n",
    "## '뉴스 기사 크롤링하기'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naver 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# chrome 드라이버 키고 naver 접속\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('http://naver.com')\n",
    "\n",
    "driver.find_element_by_id('query').send_keys('일양약품') # 검색어 입력\n",
    "driver.find_element_by_id('search_btn').click() # 검색 클릭\n",
    "\n",
    "driver.find_element_by_css_selector('div.lnb_menu').find_element_by_link_text('뉴스').click() # 뉴스 카테고리 클릭\n",
    "\n",
    "\n",
    "driver.find_element_by_id('_search_option_btn').click() # 검색조건 변경하는 옵션 켜기\n",
    "menu = driver.find_elements_by_css_selector('ul.option_menu > li.menu')\n",
    "menu[2].click()\n",
    "menu[2].find_element_by_link_text('제목').click() # 검색유형을 제목으로 변경\n",
    "\n",
    "current_window = driver.current_window_handle # 원하는 페이지를 driver주소로 설정\n",
    "\n",
    "data = {'title': [], 'date': [], 'context': []}\n",
    "for i in range(398):\n",
    "    for naver_news in driver.find_elements_by_link_text('네이버뉴스'):\n",
    "        naver_news.click()\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "        if len(soup.select('h3.tts_head')) == 0: # 아무 context가 없을 때 해당 iteration은 건너뛰기\n",
    "            driver.close()\n",
    "            driver.switch_to.window(current_window)\n",
    "            continue\n",
    "        if soup.find('div', id='articleBodyContents').script == None:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(current_window)\n",
    "            continue\n",
    "        data['title'].append(soup.select('h3.tts_head')[0].text.strip())\n",
    "        data['date'].append(soup.select('span.t11')[0].text.strip())\n",
    "        soup.find('div', id='articleBodyContents').script.extract()\n",
    "        data['context'].append(\n",
    "            soup.find('div', id='articleBodyContents').text.strip())\n",
    "        driver.close()\n",
    "        driver.switch_to.window(current_window)\n",
    "    driver.find_element_by_link_text('다음페이지').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링 중간에 오류발생 시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.switch_to.window(current_window)\n",
    "\n",
    "for i in range(400):\n",
    "    for naver_news in driver.find_elements_by_link_text('네이버뉴스'):\n",
    "        naver_news.click()\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "        if len(soup.select('h3.tts_head')) == 0: # 아무 context가 없을 때 해당 iteration은 건너뛰기\n",
    "            driver.close()\n",
    "            driver.switch_to.window(current_window)\n",
    "            continue\n",
    "        if soup.find('div', id='articleBodyContents').script == None:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(current_window)\n",
    "            continue\n",
    "        data['title'].append(soup.select('h3.tts_head')[0].text.strip())\n",
    "        data['date'].append(soup.select('span.t11')[0].text.strip())\n",
    "        soup.find('div', id='articleBodyContents').script.extract()\n",
    "        data['context'].append(\n",
    "            soup.find('div', id='articleBodyContents').text.strip())\n",
    "        driver.close()\n",
    "        driver.switch_to.window(current_window)\n",
    "    if i == 400:\n",
    "        break\n",
    "    driver.find_element_by_link_text('다음페이지').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동성제약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "df.to_csv('동성제약_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일양약품"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv('일양약품_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대웅제약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data2, columns = ['title', 'date', 'context'])\n",
    "df2.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df2.reset_index(drop = True, inplace = True)\n",
    "df2.to_csv('대웅제약_naver_news.csv')\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JW중외제약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['context'] == 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv('JW중외제약_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 부광약품"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv('부광약품_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영진약품"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv('영진약품_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일양약품"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "df.to_csv('일양약품_naver_news.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 회사에 대해 전부 크롤링하기\n",
    "\n",
    "### 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company = pd.read_csv('company_names.csv')\n",
    "company_list = set(company['name'].unique())\n",
    "company_names = company_list - set(['동성제약', '일양약품', '대웅제약', 'JW중외제약', '부광약품'])\n",
    "print('before : %d \\n' % len(company_list), 'after : %d \\n' % len(company_names))\n",
    "len(company_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = {'title': [], 'date': [], 'context': [], 'company' : []}\n",
    "# chrome 드라이버 키고 naver 접속\n",
    "for name in ['CJ', 'GS']:    \n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get('http://naver.com')\n",
    "\n",
    "\n",
    "    driver.find_element_by_id('query').send_keys('{}'.format(name)) # 검색어 입력\n",
    "    driver.find_element_by_id('search_btn').click() # 검색 클릭\n",
    "\n",
    "    driver.find_element_by_css_selector('div.lnb_menu').find_element_by_link_text('뉴스').click() # 뉴스 카테고리 클릭\n",
    "\n",
    "\n",
    "    driver.find_element_by_id('_search_option_btn').click() # 검색조건 변경하는 옵션 켜기\n",
    "    menu = driver.find_elements_by_css_selector('ul.option_menu > li.menu')\n",
    "    menu[2].click()\n",
    "    menu[2].find_element_by_link_text('제목').click() # 검색유형을 제목으로 변경\n",
    "\n",
    "    current_window = driver.current_window_handle # 원하는 페이지를 driver주소로 설정\n",
    "\n",
    "\n",
    "    for i in range(1):\n",
    "        for naver_news in driver.find_elements_by_link_text('네이버뉴스'):\n",
    "            naver_news.click()\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            html = driver.page_source\n",
    "            soup = bs(html, 'html.parser')\n",
    "            if len(soup.select('h3.tts_head')) == 0: # 아무 context가 없을 때 해당 iteration은 건너뛰기\n",
    "                driver.close()\n",
    "                driver.switch_to.window(current_window)\n",
    "                continue\n",
    "            if soup.find('div', id='articleBodyContents').script == None:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(current_window)\n",
    "                continue\n",
    "            data['company'].append(name)\n",
    "            data['title'].append(soup.select('h3.tts_head')[0].text.strip())\n",
    "            data['date'].append(soup.select('span.t11')[0].text.strip())\n",
    "            soup.find('div', id='articleBodyContents').script.extract()\n",
    "            data['context'].append(soup.find('div', id='articleBodyContents').text.strip())\n",
    "            driver.close()\n",
    "            driver.switch_to.window(current_window)\n",
    "        if i == 1:\n",
    "            break\n",
    "        driver.find_element_by_link_text('다음페이지').click()\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title', 'date', 'context', 'company'])\n",
    "df.drop_duplicates('context', keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv('naver_news_company_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 발생한 오류\n",
    "### 1. 인터넷 뉴스 회사마다 html형식이 달라 크롤링에 제약이 생겼다.\n",
    " - 해결 못 함\n",
    "\n",
    "### 2. pd.read_csv() 과정에서 'Initializing from file failed' 오류가 발생\n",
    " - pandas의 자체 버그이며 해결방법으로는 engine = 'python' 옵션을 추가해주면 된다.\n",
    " \n",
    "### 3. for구문 중간에 자꾸 list Index out of range 오류가 발생\n",
    " - 이거는 단지 select된 element내에 아무것도 없었기 때문에 [0]에서 오류가 발생\n",
    " - for구문 안에 coutinue를 추가해서 오류 발생 시 해당 iteration은 건너뛰도록 조치\n",
    " \n",
    "### 4. 크롤링 과정에서 None값이 뜨는 경우\n",
    "- 이거 또한 if구문으로 조건을 걸어 건네뛰도록 조치하엿다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로 배운 것\n",
    "### 1. Multiple class인 element 찾기\n",
    " - css_selector(.class.class)\n",
    "\n",
    "### 2. BeautifulSoup를 통해 parsing된 문서에서 id로 찾기(select 말고)\n",
    " - soup.find('tag', id = '')\n",
    " - 참고https://stackoverflow.com/questions/25614702/get-contents-of-div-by-id-with-beautifulsoup\n",
    " \n",
    "### 3. 글 중간에 있는 특정 tag를 제외한 상태로 긁어오기\n",
    " - soup.find().tag.extract() : tag를 extract()한 상태가 된다.(soup 안에서 해당 tag가 삭제됨)\n",
    " - 참고https://stackoverflow.com/questions/48877343/exclude-tags-with-beautifulsoup\n",
    " - 제외한다는 말을 'exclude'라고 표현함\n",
    " \n",
    "### 4. DataFrame 내에 중복되는 row 삭제하기\n",
    " - pd.drop_duplicates(column_label, keep = 'first')\n",
    " - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
